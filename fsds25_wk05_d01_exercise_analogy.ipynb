{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "64d28ec0",
   "metadata": {},
   "source": [
    "# Fundamentals of Social Data Science \n",
    "\n",
    "## Week 5 Day 1 Exercise: Working on Brains\n",
    "\n",
    "To work on brains you can do this through the terminal or through VS Code (or another coding IDE such as cursor, but we will focus on VS Code). I first recommend going through the terminal:\n",
    "\n",
    "You will need to be on the University VPN. Then in the terminal (wherever you see commands with `$`at the front that will be from the terminal, and `>` at front would be within the Python interpreter. You don't type the `$`, just the part afterwards.\n",
    "\n",
    "`$` `ssh brains.ox.ac.uk`\n",
    "\n",
    "The first time you do this you will be asked to enter your password. If you will be using brains a lot, I recommend using an SSH key. You can follow standard instructions for creating a key, but I would recommend waiting until after this exercise, so that you have VS Code set up. \n",
    "\n",
    "You will be presented with a new command prompt. It should look like: \n",
    "\n",
    "~~~bash\n",
    "(base) LOGIN1234@brains:~$\n",
    "~~~\n",
    "\n",
    "This is your command prompt. You can use `ls` to list files, but there shouldn't be any. `ls -a` should show some hidden files like `.bashrc`. You can (and should) check this file's contents with: \n",
    "\n",
    "`$` `cat .bashrc`\n",
    "\n",
    "And check at the bottom of it. It should have a line saying `export HF_HOME=/data/resource/huggingface`. This is important since we will want to use a shared model cache so that people do not have to download the same data twice. It will not have a similar environment variable for gensim data that we will use. You will want to set that yourself. We will do it in the code below, but you can also add this line to the same `.bashrc`: `export GENSIM_DATA_DIR=/data/resource/gensim`. After you change the `.bashrc` in any way you have to reload it: \n",
    "\n",
    "`$` `source ~/.bashrc`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8c83384",
   "metadata": {},
   "source": [
    "# Activating the envirinment. \n",
    "\n",
    "Once you have cloned the repository, you should be able to work in VS Code, which should set up a lot for you. The important thing is that you use the correct `conda` environment. It will ask you when you run this file. You should select: \n",
    "\n",
    "`/opt/anaconda/envs/fsds25-conda-env`\n",
    "\n",
    "The kernel in the upper left corner should say \"fsds25-conda-env (Python 3.12.12)\n",
    "\n",
    "On a terminal, you would type: \n",
    "`$` `conda activate /opt/anaconda/envs/fsds25-conda-env`\n",
    "\n",
    "This is a shared `conda` environment. This means everyone should be able to access the same environment, and not require any downloading or installing. \n",
    "\n",
    "Should you wish to extend this environment or add things, I would recommend that from your home directory you create your own environment with: \n",
    "\n",
    "~~~sh\n",
    "conda create --prefix <fsds25-conda-env> python=3.12\n",
    "conda install -c conda-forge sentence-transformers\n",
    "conda install gensim\n",
    "...etc\n",
    "~~~\n",
    "\n",
    "where instead of `<fsds25-conda-env>` you type your own environment name. It should be automatically stored on `/data/` and won't need much fussing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1056aede",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda/envs/fsds25-conda-env/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import gensim.downloader as api\n",
    "from scipy.spatial.distance import cosine\n",
    "import torch\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "os.environ[\"GENSIM_DATA_DIR\"] = \"/data/resource/gensim\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "19f2d53c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "GPU: NVIDIA L40S\n"
     ]
    }
   ],
   "source": [
    "# # Check if GPU is available\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Using device: {device}\")\n",
    "if device == 'cuda':\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    # from sentence_transformers import SentenceTransformer\n",
    "    device=\"cuda:1\"    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83d0ddda",
   "metadata": {},
   "source": [
    "# Part 1. Loading and doing an analogy with Word2Vec on the server\n",
    "\n",
    "First we will load word2vec. After this is done, try connecting via a terminal and typing `nvtop`. This will give an overview of the GPUs and how much VRAM is being parked. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fdade390",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Word2Vec model...\n",
      "(This may take a few minutes on first run - downloading ~1.6GB)\n",
      "✓ Word2Vec loaded\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading Word2Vec model...\")\n",
    "print(\"(This may take a few minutes on first run - downloading ~1.6GB)\")\n",
    "word2vec_model = api.load('word2vec-google-news-300')\n",
    "print(\"✓ Word2Vec loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3b648ada",
   "metadata": {},
   "outputs": [],
   "source": [
    "def word2vec_analogy(model, word_a, word_b, word_c, top_n=5):\n",
    "    \"\"\"\n",
    "    Perform vector arithmetic: word_a - word_b + word_c\n",
    "    Returns top_n most similar words to the resulting vector.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Get vectors\n",
    "        vec_a = model[word_a]\n",
    "        vec_b = model[word_b]\n",
    "        vec_c = model[word_c]\n",
    "        \n",
    "        # Vector arithmetic\n",
    "        result_vec = vec_c - vec_b + vec_a\n",
    "        \n",
    "        # Find most similar (excluding input words)\n",
    "        similar = model.similar_by_vector(result_vec, topn=top_n + 3)\n",
    "        \n",
    "        # Filter out input words\n",
    "        filtered = [(word, score) for word, score in similar \n",
    "                   if word.lower() not in [word_a.lower(), word_b.lower(), word_c.lower()]]\n",
    "        \n",
    "        return filtered[:top_n]\n",
    "    \n",
    "    except KeyError as e:\n",
    "        return f\"Word not in vocabulary: {e}\"\n",
    "\n",
    "def print_analogy_results(word_a, word_b, word_c, results):\n",
    "    \"\"\"\n",
    "    Pretty print analogy results\n",
    "    \"\"\"\n",
    "    print(f\"\\n{word_a}:{word_b} :: {word_c}:?\")\n",
    "    print(\"=\"*50)\n",
    "    if isinstance(results, str):\n",
    "        print(results)\n",
    "    else:\n",
    "        for i, (word, score) in enumerate(results, 1):\n",
    "            print(f\"{i}. {word:20s} (similarity: {score:.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5c6a5799",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "WORD2VEC: Classic 'king - man + woman' analogy\n",
      "============================================================\n",
      "\n",
      "woman:man :: king:?\n",
      "==================================================\n",
      "1. queen                (similarity: 0.7301)\n",
      "2. monarch              (similarity: 0.6455)\n",
      "3. princess             (similarity: 0.6156)\n",
      "4. crown_prince         (similarity: 0.5819)\n",
      "5. prince               (similarity: 0.5777)\n",
      "6. kings                (similarity: 0.5614)\n",
      "7. sultan               (similarity: 0.5377)\n",
      "8. Queen_Consort        (similarity: 0.5344)\n",
      "9. queens               (similarity: 0.5290)\n",
      "10. ruler                (similarity: 0.5247)\n",
      "\n",
      "→ 'queen' found at rank 1\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"WORD2VEC: Classic 'king - man + woman' analogy\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "results_w2v = word2vec_analogy(word2vec_model, 'woman', 'man', 'king', top_n=10)\n",
    "print_analogy_results('woman', 'man', 'king', results_w2v)\n",
    "\n",
    "# Check if 'queen' is in top results\n",
    "if isinstance(results_w2v, list):\n",
    "    queen_rank = next((i for i, (word, _) in enumerate(results_w2v, 1) \n",
    "                      if 'queen' in word.lower()), None)\n",
    "    if queen_rank:\n",
    "        print(f\"\\n→ 'queen' found at rank {queen_rank}\")\n",
    "    else:\n",
    "        print(f\"\\n→ 'queen' not in top 10 results!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e58955e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "WORD2VEC:\n",
      "\n",
      "Paris:France :: London:?\n",
      "==================================================\n",
      "1. Londons              (similarity: 0.5469)\n",
      "2. Islamabad_Slyvia_Hui (similarity: 0.5463)\n",
      "3. Canary_Wharf         (similarity: 0.5453)\n",
      "4. Canary_Warf          (similarity: 0.5428)\n",
      "5. EURASIAN_NATURAL_RESOURCES_CORP. (similarity: 0.5356)\n"
     ]
    }
   ],
   "source": [
    "# A custom analogy with Word2Vec\n",
    "word_a = 'Paris'\n",
    "word_b = 'France' \n",
    "word_c = 'London'\n",
    "\n",
    "print(\"\\nWORD2VEC:\")\n",
    "results = word2vec_analogy(word2vec_model, word_a, word_b, word_c, top_n=5)\n",
    "print_analogy_results(word_a, word_b, word_c, results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "49b93f07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "TESTING POLYSEMY: 'bank' (financial vs. river)\n",
      "============================================================\n",
      "\n",
      "WORD2VEC: river:water :: bank:?\n",
      "\n",
      "water:river :: bank:?\n",
      "==================================================\n",
      "1. banks                (similarity: 0.4827)\n",
      "2. banking              (similarity: 0.4743)\n",
      "3. mortgage_lender      (similarity: 0.4391)\n",
      "4. lender               (similarity: 0.4280)\n",
      "5. BofA_NYSE_BAC        (similarity: 0.4224)\n",
      "\n",
      "------------------------------------------------------------\n",
      "WORD2VEC: money:finance :: bank:?\n",
      "\n",
      "finance:money :: bank:?\n",
      "==================================================\n",
      "1. banking              (similarity: 0.6904)\n",
      "2. lender               (similarity: 0.5620)\n",
      "3. banks                (similarity: 0.5395)\n",
      "4. banker               (similarity: 0.5368)\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TESTING POLYSEMY: 'bank' (financial vs. river)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Word2Vec: only has one vector for 'bank'\n",
    "print(\"\\nWORD2VEC: river:water :: bank:?\")\n",
    "results_w2v_bank = word2vec_analogy(word2vec_model, 'water', 'river', 'bank', top_n=5)\n",
    "print_analogy_results('water', 'river', 'bank', results_w2v_bank)\n",
    "\n",
    "print(\"\\n\" + \"-\"*60)\n",
    "print(\"WORD2VEC: money:finance :: bank:?\")\n",
    "results_w2v_bank2 = word2vec_analogy(word2vec_model, 'finance', 'money', 'bank', top_n=5)\n",
    "print_analogy_results('finance', 'money', 'bank', results_w2v_bank2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4698200c",
   "metadata": {},
   "source": [
    "# Part 2. Loading and doing an analogy with BERT \n",
    "\n",
    "This part is in fact a little more tricky considering the way embeddings work in BERT. You will see below a few strategies. The most interesting of these, in my opinion, is the code that explores which prompt is most likely to give the appropriate result. \n",
    "\n",
    "Hopefully, this code will allow you to draw upon the existing models downloaded to `Brains`. If your code suggests that you are downloading your own models, then please notify the instructor as the permissions might not be set appropriately. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1e0d234c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "model_path = '/data/resource/huggingface/models--sentence-transformers--all-mpnet-base-v2/snapshots/12e86a3c702fc3c50205a8db88f0ec7c0b6b94a0'\n",
    "sbert_model = SentenceTransformer(model_path, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f9cff28f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def contextualized_analogy(model, word_a, word_b, word_c, candidates, context_template=None):\n",
    "    \"\"\"\n",
    "    Perform analogy using contextualized embeddings.\n",
    "    \n",
    "    Args:\n",
    "        model: SentenceTransformer model\n",
    "        word_a, word_b, word_c: The analogy words (A:B :: C:?)\n",
    "        candidates: List of candidate words for the answer\n",
    "        context_template: Optional template for embedding words in context\n",
    "    \"\"\"\n",
    "    # Default: simple sentence context\n",
    "    if context_template is None:\n",
    "        context_template = \"The word {} is used in this sentence.\"\n",
    "    \n",
    "    # Get contextualized embeddings\n",
    "    emb_a = model.encode(context_template.format(word_a))\n",
    "    emb_b = model.encode(context_template.format(word_b))\n",
    "    emb_c = model.encode(context_template.format(word_c))\n",
    "    \n",
    "    # Vector arithmetic in contextualized space\n",
    "    target_vec = emb_c - emb_b + emb_a\n",
    "    \n",
    "    # Compare to candidates\n",
    "    results = []\n",
    "    for candidate in candidates:\n",
    "        emb_candidate = model.encode(context_template.format(candidate))\n",
    "        similarity = 1 - cosine(target_vec, emb_candidate)\n",
    "        results.append((candidate, similarity))\n",
    "    \n",
    "    # Sort by similarity\n",
    "    results.sort(key=lambda x: x[1], reverse=True)\n",
    "    return results\n",
    "\n",
    "def print_contextual_results(word_a, word_b, word_c, results):\n",
    "    \"\"\"\n",
    "    Pretty print contextualized analogy results\n",
    "    \"\"\"\n",
    "    print(f\"\\n{word_a}:{word_b} :: {word_c}:?\")\n",
    "    print(\"=\"*50)\n",
    "    for i, (word, score) in enumerate(results, 1):\n",
    "        print(f\"{i}. {word:20s} (similarity: {score:.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ce511a06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "SENTENCE-BERT: Contextualized 'woman:man :: king:?' analogy\n",
      "============================================================\n",
      "\n",
      "woman:man :: king:?\n",
      "==================================================\n",
      "1. queen                (similarity: 0.7897)\n",
      "2. monarch              (similarity: 0.6850)\n",
      "3. princess             (similarity: 0.6701)\n",
      "4. royal                (similarity: 0.6201)\n",
      "5. kingdom              (similarity: 0.6022)\n",
      "6. throne               (similarity: 0.5996)\n",
      "7. ruler                (similarity: 0.5034)\n",
      "8. emperor              (similarity: 0.5014)\n",
      "9. prince               (similarity: 0.4598)\n",
      "10. castle               (similarity: 0.4598)\n",
      "\n",
      "→ 'queen' found at rank 1\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"SENTENCE-BERT: Contextualized 'woman:man :: king:?' analogy\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Define candidates (including expected answer + distractors)\n",
    "candidates = ['queen', 'monarch', 'prince', 'princess', 'throne', \n",
    "              'ruler', 'kingdom', 'royal', 'emperor', 'castle']\n",
    "\n",
    "results_sbert = contextualized_analogy(sbert_model, 'woman', 'man', 'king', candidates)\n",
    "print_contextual_results('woman', 'man', 'king', results_sbert[:10])\n",
    "\n",
    "# Check queen's rank\n",
    "queen_rank = next((i for i, (word, _) in enumerate(results_sbert, 1) \n",
    "                  if word.lower() == 'queen'), None)\n",
    "if queen_rank:\n",
    "    print(f\"\\n→ 'queen' found at rank {queen_rank}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "46deb0cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "SENTENCE-BERT: Contextualized 'bank' analogies\n",
      "============================================================\n",
      "\n",
      "Context: 'The bank approved my loan application.'\n",
      "\n",
      "finance:money :: bank:?\n",
      "==================================================\n",
      "1. lender               (similarity: 0.9562)\n",
      "2. institution          (similarity: 0.9325)\n",
      "3. credit               (similarity: 0.9109)\n",
      "4. company              (similarity: 0.9049)\n",
      "5. branch               (similarity: 0.8854)\n",
      "6. slope                (similarity: 0.7768)\n",
      "7. shore                (similarity: 0.7531)\n",
      "8. edge                 (similarity: 0.6958)\n",
      "\n",
      "------------------------------------------------------------\n",
      "Context: 'We sat on the bank of the river.'\n",
      "\n",
      "water:river :: bank:?\n",
      "==================================================\n",
      "1. water                (similarity: 0.9877)\n",
      "2. shore                (similarity: 0.9381)\n",
      "3. side                 (similarity: 0.9348)\n",
      "4. edge                 (similarity: 0.9182)\n",
      "5. branch               (similarity: 0.9019)\n",
      "6. slope                (similarity: 0.8765)\n",
      "7. lender               (similarity: 0.8752)\n",
      "8. institution          (similarity: 0.8598)\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"SENTENCE-BERT: Contextualized 'bank' analogies\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Financial context\n",
    "print(\"\\nContext: 'The bank approved my loan application.'\")\n",
    "financial_template = \"The bank approved my loan application, just like {}.\"\n",
    "financial_candidates = ['institution', 'lender', 'company', 'branch', \n",
    "                       'shore', 'edge', 'slope', 'credit']\n",
    "\n",
    "results_bank_financial = contextualized_analogy(\n",
    "    sbert_model, 'finance', 'money', 'bank', \n",
    "    financial_candidates, context_template=financial_template\n",
    ")\n",
    "print_contextual_results('finance', 'money', 'bank', results_bank_financial)\n",
    "\n",
    "# River context\n",
    "print(\"\\n\" + \"-\"*60)\n",
    "print(\"Context: 'We sat on the bank of the river.'\")\n",
    "river_template = \"We sat on the bank of the river, near the {}.\"\n",
    "river_candidates = ['shore', 'edge', 'slope', 'side', \n",
    "                   'institution', 'lender', 'branch', 'water']\n",
    "\n",
    "results_bank_river = contextualized_analogy(\n",
    "    sbert_model, 'water', 'river', 'bank', \n",
    "    river_candidates, context_template=river_template\n",
    ")\n",
    "print_contextual_results('water', 'river', 'bank', results_bank_river)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6395ed92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "SENTENCE-BERT:\n",
      "\n",
      "Paris:France :: London:?\n",
      "==================================================\n",
      "1. city                 (similarity: 0.6386)\n",
      "2. England              (similarity: 0.3225)\n",
      "3. UK                   (similarity: 0.3207)\n",
      "4. Britain              (similarity: 0.3089)\n",
      "5. Europe               (similarity: 0.2674)\n",
      "6. country              (similarity: 0.2060)\n"
     ]
    }
   ],
   "source": [
    "# Your custom analogy with Sentence-BERT\n",
    "candidates = ['England', 'Britain', 'UK', 'Europe', 'city', 'country']\n",
    "\n",
    "print(\"\\nSENTENCE-BERT:\")\n",
    "results = contextualized_analogy(sbert_model, word_a, word_b, word_c, candidates)\n",
    "print_contextual_results(word_a, word_b, word_c, results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdb6c81c",
   "metadata": {},
   "source": [
    "# Part 3. Other approaches with BERT \n",
    "\n",
    "The above code used sentenceBERT with comparative sentence embeddings where we changed the words. However, with BERT we can (and usually) do a masked token within a sentence, like `\"London is to England what [MASK] is to France\"`.\n",
    "\n",
    "These are just some interesting ways to explore analogies within transformer models and can be used to explore your topic. For example, we look at: \n",
    "- what difference a prompt makes\n",
    "- what about different models\n",
    "- what about the analogy in a different order\n",
    "\n",
    "Try all three, experiement with different analogies and even explore different models avaiable on `/data/resources/huggingface/`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "630f087f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-large-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def bert_analogy_probabilities(word_a, word_b, word_c, tokenizer, model, top_k=10, prompt_style=\"default\"):\n",
    "    \"\"\"\n",
    "    Get top-k word predictions using masked language modeling.\n",
    "    \"\"\"\n",
    "    prompts = {\n",
    "        \"default\": f\"{word_a} is to {word_b} as {word_c} is to [MASK].\",\n",
    "        \"analogy\": f\"{word_a}:{word_b} :: {word_c}:[MASK]\",\n",
    "        \"relationship\": f\"The relationship between {word_a} and {word_b} is like the relationship between {word_c} and [MASK].\",\n",
    "        \"similar\": f\"{word_a} relates to {word_b} like {word_c} relates to [MASK].\",\n",
    "        \"minimal\": f\"{word_a} {word_b} {word_c} [MASK]\"\n",
    "    }\n",
    "    \n",
    "    text = prompts.get(prompt_style, prompts[\"default\"])\n",
    "    \n",
    "    inputs = tokenizer(text, return_tensors=\"pt\").to(device)\n",
    "    mask_token_index = torch.where(inputs[\"input_ids\"] == tokenizer.mask_token_id)[1]\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "    \n",
    "    mask_token_logits = logits[0, mask_token_index, :]\n",
    "    probs = torch.softmax(mask_token_logits, dim=-1)\n",
    "    top_k_tokens = torch.topk(probs, top_k, dim=1)\n",
    "    \n",
    "    results = []\n",
    "    for token_id, prob in zip(top_k_tokens.indices[0], top_k_tokens.values[0]):\n",
    "        word = tokenizer.decode([token_id])\n",
    "        results.append((word.strip(), prob.item()))\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Load model once\n",
    "\n",
    "model_name = \"bert-large-uncased\"          # 340M params\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForMaskedLM.from_pretrained(model_name).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed8aa53b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# 3.1. Different prompting strategies\n",
    "# ============================================\n",
    "print(\"=\" * 70)\n",
    "print(\"EXPERIMENT 1: Different Prompt Styles\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "word_a, word_b, word_c = \"king\", \"queen\", \"man\"\n",
    "\n",
    "for prompt_style in [\"default\", \"analogy\", \"relationship\", \"similar\", \"minimal\"]:\n",
    "    print(f\"\\n{prompt_style.upper()} style:\")\n",
    "    print(f\"{word_a}:{word_b} :: {word_c}:?\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    results = bert_analogy_probabilities(word_a, word_b, word_c, tokenizer, model, \n",
    "                                        top_k=5, prompt_style=prompt_style)\n",
    "    \n",
    "    for i, (word, prob) in enumerate(results, 1):\n",
    "        print(f\"  {i}. {word:15s} {prob:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbc44c8e",
   "metadata": {},
   "source": [
    "## 3.2 Comparing models \n",
    "\n",
    "In the code below, we can compare different BERT models. All of these are uncased, which explains `london` rather than `London`. Note that if you were to scale this up, it might be more suitable to try all the analogies per model rather than alternate between them, though on Brains all of these should be stored in VRAM with little issue. \n",
    "\n",
    "I tried a few of the other models on the server and sadly, several require further dependencies not featured in the `conda` environment and led to some challenges with dependencies (such as Microsoft's latest \"DeBERTa\" model). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4c000fb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "======================================================================\n",
      "EXPERIMENT 2: Compare Multiple Models\n",
      "======================================================================\n",
      "\n",
      "Analogy: paris:france :: berlin:?\n",
      "======================================================================\n",
      "\n",
      "bert-base-uncased:\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1. germany         0.7205\n",
      "  2. britain         0.0642\n",
      "  3. austria         0.0356\n",
      "  4. england         0.0354\n",
      "  5. russia          0.0351\n",
      "\n",
      "distilbert-base-uncased:\n",
      "--------------------------------------------------\n",
      "  1. germany         0.4331\n",
      "  2. england         0.0733\n",
      "  3. belgium         0.0700\n",
      "  4. italy           0.0624\n",
      "  5. britain         0.0509\n",
      "\n",
      "bert-large-uncased:\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-large-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1. germany         0.9626\n",
      "  2. berlin          0.0061\n",
      "  3. russia          0.0059\n",
      "  4. prussia         0.0058\n",
      "  5. europe          0.0023\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# 3.2. Compare multiple models\n",
    "# ============================================\n",
    "print(\"\\n\\n\" + \"=\" * 70)\n",
    "print(\"EXPERIMENT 2: Compare Multiple Models\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "model_names = [\n",
    "    \"bert-base-uncased\",\n",
    "    \"distilbert-base-uncased\",\n",
    "    \"bert-large-uncased\"\n",
    "]\n",
    "\n",
    "word_a, word_b, word_c = \"paris\", \"france\", \"berlin\"\n",
    "\n",
    "print(f\"\\nAnalogy: {word_a}:{word_b} :: {word_c}:?\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for model_name in model_names:\n",
    "    print(f\"\\n{model_name}:\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    tok = AutoTokenizer.from_pretrained(model_name)\n",
    "    mdl = AutoModelForMaskedLM.from_pretrained(model_name).to(device)\n",
    "    \n",
    "    results = bert_analogy_probabilities(word_a, word_b, word_c, tok, mdl, top_k=5)\n",
    "    \n",
    "    for i, (word, prob) in enumerate(results, 1):\n",
    "        print(f\"  {i}. {word:15s} {prob:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b8368fd",
   "metadata": {},
   "source": [
    "## 3.3 Bidirectional Analogies\n",
    "\n",
    "`Man:Woman::King:Queen` often assumes symmetry, such that if this works we should be able to go `King:Queen::Man:Woman`. But yet, we are comparing vectors and their multiplication is not commutative. So there is really no guarantee that this will work in reverse. \n",
    "\n",
    "Have a look at some of the examples below to see this in action. \n",
    "\n",
    "Also notice that I added the parameter, `prompt_style=\"relationship\"`. This is because above it seemed that this prompt style was the most accurate. But that might be contextual and not guaranteed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "792bca58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "======================================================================\n",
      "EXPERIMENT 3: Bidirectional Analogy Checking\n",
      "======================================================================\n",
      "\n",
      "king:queen :: man:woman\n",
      "  Forward (man→woman): 0.8028\n",
      "  Backward (king→queen): 0.9369\n",
      "  Consistency: 0.8699\n",
      "\n",
      "  Top 5 predictions (forward):\n",
      "    ✓ 1. woman           0.8028\n",
      "      2. beast           0.0655\n",
      "      3. wife            0.0170\n",
      "      4. animal          0.0118\n",
      "      5. horse           0.0103\n",
      "\n",
      "  Top 5 predictions (backward):\n",
      "    ✓ 1. queen           0.9369\n",
      "      2. king            0.0128\n",
      "      3. country         0.0046\n",
      "      4. emperor         0.0044\n",
      "      5. lady            0.0025\n",
      "\n",
      "paris:france :: london:england\n",
      "  Forward (london→england): 0.0000\n",
      "  Backward (paris→france): 0.0000\n",
      "  Consistency: 0.0000\n",
      "\n",
      "  Top 5 predictions (forward):\n",
      "      1. paris           0.1583\n",
      "      2. manchester      0.0633\n",
      "      3. london          0.0590\n",
      "      4. brussels        0.0580\n",
      "      5. birmingham      0.0435\n",
      "\n",
      "  Top 5 predictions (backward):\n",
      "      1. paris           0.1859\n",
      "      2. brussels        0.1217\n",
      "      3. rome            0.0983\n",
      "      4. london          0.0770\n",
      "      5. madrid          0.0482\n",
      "\n",
      "dog:puppy :: cat:kitten\n",
      "  Forward (cat→kitten): 0.0054\n",
      "  Backward (dog→puppy): 0.0138\n",
      "  Consistency: 0.0096\n",
      "\n",
      "  Top 5 predictions (forward):\n",
      "      1. mouse           0.7940\n",
      "      2. cat             0.0461\n",
      "      3. dog             0.0342\n",
      "      4. fox             0.0158\n",
      "      5. canary          0.0083\n",
      "\n",
      "  Top 5 predictions (backward):\n",
      "      1. cat             0.2494\n",
      "      2. dog             0.1485\n",
      "      3. wolf            0.0652\n",
      "      4. mouse           0.0355\n",
      "      5. sheep           0.0333\n",
      "\n",
      "big:bigger :: small:smaller\n",
      "  Forward (small→smaller): 0.0011\n",
      "  Backward (big→bigger): 0.0000\n",
      "  Consistency: 0.0006\n",
      "\n",
      "  Top 5 predictions (forward):\n",
      "      1. small           0.4728\n",
      "      2. large           0.2054\n",
      "      3. big             0.1434\n",
      "      4. medium          0.1031\n",
      "      5. little          0.0171\n",
      "\n",
      "  Top 5 predictions (backward):\n",
      "      1. small           0.8092\n",
      "      2. little          0.1057\n",
      "      3. medium          0.0256\n",
      "      4. big             0.0159\n",
      "      5. tiny            0.0027\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# 3. Bidirectional analogy checking\n",
    "# ============================================\n",
    "print(\"\\n\\n\" + \"=\" * 70)\n",
    "print(\"EXPERIMENT 3: Bidirectional Analogy Checking\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "def bidirectional_analogy(word_a, word_b, word_c, word_d, tokenizer, model, prompt_style=None):\n",
    "    \"\"\"\n",
    "    Test both directions: A:B::C:D and C:D::A:B\n",
    "    \"\"\"\n",
    "    # Forward: A:B::C:?\n",
    "    if not prompt_style: \n",
    "        prompt_stlye=\"default\"\n",
    "        \n",
    "    forward_results = bert_analogy_probabilities(word_a, word_b, word_c, tokenizer, model, top_k=20, prompt_style=prompt_style)\n",
    "    forward_dict = {w: p for w, p in forward_results}\n",
    "    \n",
    "    # Backward: C:D::A:?\n",
    "    backward_results = bert_analogy_probabilities(word_c, word_d, word_a, tokenizer, model, top_k=20, prompt_style=prompt_style)\n",
    "    backward_dict = {w: p for w, p in backward_results}\n",
    "    \n",
    "    forward_prob_d = forward_dict.get(word_d, 0.0)\n",
    "    backward_prob_b = backward_dict.get(word_b, 0.0)\n",
    "    \n",
    "    consistency = (forward_prob_d + backward_prob_b) / 2\n",
    "    \n",
    "    print(f\"\\n{word_a}:{word_b} :: {word_c}:{word_d}\")\n",
    "    print(f\"  Forward ({word_c}→{word_d}): {forward_prob_d:.4f}\")\n",
    "    print(f\"  Backward ({word_a}→{word_b}): {backward_prob_b:.4f}\")\n",
    "    print(f\"  Consistency: {consistency:.4f}\")\n",
    "    \n",
    "    print(f\"\\n  Top 5 predictions (forward):\")\n",
    "    for i, (word, prob) in enumerate(forward_results[:5], 1):\n",
    "        marker = \"✓\" if word == word_d else \" \"\n",
    "        print(f\"    {marker} {i}. {word:15s} {prob:.4f}\")\n",
    "    \n",
    "    print(f\"\\n  Top 5 predictions (backward):\")\n",
    "    for i, (word, prob) in enumerate(backward_results[:5], 1):\n",
    "        marker = \"✓\" if word == word_b else \" \"\n",
    "        print(f\"    {marker} {i}. {word:15s} {prob:.4f}\")\n",
    "    \n",
    "    return consistency, forward_results, backward_results\n",
    "\n",
    "# Test several analogies\n",
    "test_cases = [\n",
    "    (\"king\", \"queen\", \"man\", \"woman\"),\n",
    "    (\"paris\", \"france\", \"london\", \"england\"),\n",
    "    (\"dog\", \"puppy\", \"cat\", \"kitten\"),\n",
    "    (\"big\", \"bigger\", \"small\", \"smaller\"),\n",
    "]\n",
    "\n",
    "for word_a, word_b, word_c, word_d in test_cases:\n",
    "    consistency, _, _ = bidirectional_analogy(word_a, \n",
    "                                              word_b, \n",
    "                                              word_c, \n",
    "                                              word_d, \n",
    "                                              tokenizer, \n",
    "                                              model, \n",
    "                                              prompt_style=\"relationship\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fsds25-conda-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
